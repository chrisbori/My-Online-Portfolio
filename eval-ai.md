| [Home Page](https://chrisbori.github.io/My-Online-Portfolio/) | [Eval-AI Model Check](https://chrisbori.github.io/My-Online-Portfolio/eval-ai) | [Puerto Rico's Act 22](https://chrisbori.github.io/My-Online-Portfolio/final-project-part-three) | [Visualizing Debt](https://chrisbori.github.io/My-Online-Portfolio/visualizing-government-debt) | [Everybody Poops](https://chrisbori.github.io/My-Online-Portfolio/critique-by-design) |

![EvalAI Logo](https://github.com/user-attachments/assets/ef02be1d-131c-4b2d-b3cf-876a6b9705a7)

# **Finalist** for the McGnnis Social Enterprise Venture Competition

We turned a first draft prototype from our Policy Innovation Lab course with Bloomberg Government as a client into a startup venture for the McGinnis competition, in which we placed as finalists. What started as a class project grew into something bigger. Read more about the original prototype below. Click [here](#my-heading) to jump to our competition pitch.

# Eval-AI Model Check

![Screenshot 2025-05-14 134153](https://github.com/user-attachments/assets/e01268de-fca2-4eab-8e27-21b023ddaaf4)

**As Team Lead for the Eval-AI project**, I worked alongside four team members for **Bloomberg Government** in the Policy Innovation Lab course to apply design thinking principles to a business problem. We designed an open-source compliance evaluation framework for Generative AI models via an [online interface](https://www.figma.com/proto/RscbuGdIkKUeYIRttQDe3y/PIL-Prototype-Design-(FINAL)?node-id=19-2&node-type=canvas&t=v6Gvf3JQeqKgChfB-1&scaling=min-zoom&content-scaling=fixed&page-id=0%3A1&starting-point-node-id=19%3A2) and GitHub benchmark models. The framework aims to mitigate risks like hallucinations and prompt misinterpretations in public interest AI applications by assessing AI systems against U.S. AI regulations and responsible AI principles.

### Check out the [online interface](https://www.figma.com/proto/RscbuGdIkKUeYIRttQDe3y/PIL-Prototype-Design-(FINAL)?node-id=19-2&node-type=canvas&t=v6Gvf3JQeqKgChfB-1&scaling=min-zoom&content-scaling=fixed&page-id=0%3A1&starting-point-node-id=19%3A2)! 

## Problem Statement
**There is too much inconclusiveness around how to properly mitigate the risk of using GenAI for the public interest to ensure proper interpretation and response accuracy.** The underlying issue of this statement is a high risk of hallucinations and prompt misinterpretations in public interest GenAI. 

## Proposed Solution
An online interface that would enable firms to evaluate their GenAI models on national US AI regulations and Responsible AI principles using **benchmark models**. Benchmark models are standardized sets of models/datasets used to evaluate the performance of GenAI across tasks and metrics. They serve as a reference point for comparing models.

![image](https://github.com/user-attachments/assets/0cc32252-9eb5-4233-8e76-85555026d221)

## Our Approach
Generative AI, and particularly LLMs, have hit the public and private sectors by storm, entailing a lieu of new security and testing best practices yet to be discovered. Implementing strategies that mitigate the risk of using GenAI solutions offers both a prescriptive and proactive approach to identifying gaps and vulnerabilities in Bloomberg's AI technologies. However, the public interest technology sector does not have a standard/trusted guideline as to how these strategies. Moreover, some firms within the sector may not have the technical acumen/resources to implement such strategies.  

![image](https://github.com/user-attachments/assets/ae022cf9-adb3-4e36-83be-508a30ae11a9)

Using the risk mitigation framework above, our solution focuses on reporting, specifically compliance reporting, within the iterative evaluation process for GenAI. As a result, our solution is one step in the process of mitigating the risks of hallucinations and prompt misinterpretations in GenAI models. However, we also developed a knowledge base of red teaming for GenAI before identifying our final solution. 

## Project Audience

Our primary audience is Bloomberg Gov, however, we want our solution to guide other LLM users of similar functionalities. In this way, our solution will provide a foundation for compliance scoring and reporting that can apply to LLM use cases across different companies and sectors. The larger audience for our solution are firms working with public interest GenAI. 

## Team Roles and Responsibilities

**Team Lead:** Christian Andino Borrero
Own the delivery of the project as a whole. Oversee the project timeline, delegate tasks, and ensure clear communication between team members and stakeholders. Facilitate meetings, set deadlines, and track progress to keep the project on schedule. 

**User Research Lead:** Vashishth Doshi

Own the user research strategy and protocol. Gather insights into user needs, pain points, and expectations from AI systems to ensure red teaming strategies align with user concerns. Analyze feedback and integrate findings into the project deliverables.

**Policy & Compliance Specialist:** Lisa Menda

Research relevant regulations and compliance requirements for the use of Bloomberg Gov LLM. Ensure the testing methods are consistent with both internal policies and external regulations.

**GenAI/LLM Test Designer:** Grace Sam

Own the design of LLM/GenAI testing method. Identify potential weaknesses in AI systems and outline tests that reveal these vulnerabilities. Develop protocols for assessing and improving system resilience to various threats. Focus on strategies to protect user data during testing and in production as well as identifying/preventing biases in AI outputs, particularly discriminatory or offensive responses. Identify privacy risks and help develop red teaming techniques to safeguard sensitive information. Ensure compliance with data privacy laws and guidelines in all test scenarios and provide input on data handling protocols.

**Data Security & Ethics Specialist:** Zachary Zwijacz

Focus on strategies to protect user data during testing and in production as well as identifying/preventing biases in AI outputs, particularly discriminatory or offensive responses. Identify privacy risks and help develop red teaming techniques to safeguard sensitive information. Ensure compliance with data privacy laws and guidelines in all test scenarios and provide input on data handling protocols.



# McGnnis Social Enterprise Venture Competition Pitch <a id="my-heading"></a>

![1742419645477](https://github.com/user-attachments/assets/0c539379-2384-4e0d-91e0-7d6a75dab16a)

## Market Opportunity & Problem Statement

The LLM market is projected to grow from 1.5 million users in 2023 to 259 million by 2030, with 750 million applications expected to integrate LLMs by 2025. As AI adoption accelerates, organizations face increasing regulatory scrutiny due to risks such as hallucinations, bias, and compliance failures. **Current compliance processes are fragmented, manual, and reactive, exposing companies to legal fines, reputational damage, and regulatory penalties—with non-compliance costs reaching up to $1 trillion annually.**

Enterprises in regulated industries such as finance, healthcare, legal, and insurance face the highest risk, with 50,000+ organizations struggling to align AI models with evolving regulations. AI developers, responsible for model integrity, lack standardized compliance frameworks. Meanwhile, government regulators need reliable assessment tools to enforce AI laws effectively. Despite these urgent needs, **no comprehensive compliance testing solution exists today**.

### EVAL-AI: The Compliance Solution for AI Models

EVAL-AI is an automated AI compliance testing framework designed to evaluate LLM applications against AI regulations, industry guidelines, and Responsible AI principles. EVAL-AI provides end-to-end regulatory validation for enterprises, developers, and policymakers. 

EVAL-AI ensures that AI applications meet legal, ethical, and operational compliance standards before deployment—reducing corporate liability, safeguarding reputation, and preventing costly regulatory actions.

### Why Invest in Us?

We are the right team because we combine deep expertise in AI policy, regulatory compliance, and business strategy, ensuring a holistic approach to AI evaluation. Our interdisciplinary backgrounds allow us to create a product that is aligned with emerging global AI needs. We are passionate about responsible AI and committed to building a scalable solution that not only meets industry needs but also drives meaningful social impact.
With the right investment and resources, we are confident in our ability to bring this vision to life.

## Product Maintenance

The product is a sum of parts which are explained in detail below.

‘Benchmarks’ are technical goal-posts, defined and determined with academic research and updated for newer technology and regulations. ‘Documentation’ makes our entire research process and product implementation accessible to understand and interpret, to the AI and regulation community. This keeps our methods updated and community-forward. Our ‘data’ and ‘tools’ include the evaluation code scripts and model data that we house on the GitHub repository. These enable the technological implementation of our evaluation for our clients.

**Benchmarks:**
* Create submodules for each regulatory requirement category: robustness, fairness, privacy, interpretability, transparency, and environmental impact.
* For example, FCRA regulations require accurate, fair and private use of consumer information. As banks include AI tools in their day-to-day, they will require these tools to meet such standard regulations already in place.

**Requirements:**
* Maintain clear mappings of U.S. required regulatory guidelines to measurable technology benchmarks.
* In our example, academic and technical research contribute to an FCRA compliant AI model - the skeleton will become a benchmark that a client’s model will be tested against.

**Documentation:** Provide guides for developers, contributors, and legal professionals.

**Data:** Use curated datasets for benchmark testing while respecting privacy and copyright constraints.

**Tools:**
The envisioned seamless Eval-AI tool will use the ‘Benchmarks’,  the ‘Requirements’ and the ‘Data’ discussed above in the backend. For the user it will be a simple ‘input’ the model and the 'compliance report’ output.

### Tracking User Interactions and Feedback

We have created a user interactions workflow that ensures Eval-AI remains a valuable, enterprise-ready compliance tool. Our commitment to continuous improvement ensures that Eval-AI is not just a static solution but a responsive, evolving platform that adapts to regulatory changes and user needs, making AI compliance more accessible, efficient, and effective.

**Step 1:** Collect User Interaction Data
* Analytics Dashboard – Track how users interact with Eval-AI (which features they use, where they get stuck).
* Error Logs & Usage Trends – Identify frequent issues (e.g., users abandoning compliance tests).
* Heatmaps & Click Tracking – See which buttons, reports, or sections users engage with most.

**Step 2:** Gather Direct User Feedback
* Surveys & Feedback Forms – Send short forms after users complete compliance evaluations.
* Customer Support Requests – Log common questions or complaints from early users.

**Step 3:** Analyze & Apply Insights for Product Improvement
* Identify Trends – Are users struggling with setup? Do they want more detailed compliance reports?
* Prioritize Fixes & Features – Create a backlog of improvements based on user feedback.
* Iterate & Test – Roll out updates, test changes with a small group of users, and measure if problems improve.

**Step 4:** Close the Loop with Users
* User Updates & Release Notes – Inform users about improvements based on their feedback.
* Ongoing Feedback Cycles – Keep checking in with early adopters to refine the tool.

## Problem Sizing

**Per Instance Sizing:** Each GenAI compliance failure can cost $100,000 to $60 million, depending on severity. AI-generated misinformation incurs $100,000–$5 million in reputational and legal damages. Regulatory fines, such as those under the EU AI Act, can reach 6% of annual revenue (e.g., $60 million for OpenAI). AI-driven bias lawsuits have resulted in settlements of up to $50 million. (Sources: McKinsey AI Risk Report 2024, FTC AI Compliance Guidelines, U.S. DOJ AI Ethics Investigations)

**Per Actor, Annually:** Over 70% of large enterprises use GenAI, with financial, healthcare, and tech industries facing the highest compliance risks. AI governance costs average $3 million per company per year, with top AI firms facing 1–5 compliance lawsuits annually, costing $10–$50 million per case. (Sources: PwC AI Business Adoption Report 2024, Gartner AI Risk Report 2023)

**Total Impact, Annually:** More than 100,000 companies globally deploy GenAI in high-risk domains. AI non-compliance leads to $500B+ in misinformation and fraud losses, $10B+ in regulatory fines and legal fees, and $250B in compliance audits and model validation costs. (Sources: World Economic Forum AI Risk Report 2024, AI Ethics & Compliance Global Report 2023, IDC AI Industry Forecast 2024

## Target Market Segments

### Key Market Segment 

**Regulated Enterprises** (Finance, Healthcare, Legal, Insurance)

* **Estimated Market Size:** ~50,000 enterprises globally.
* **Growth Rate:** 20% annually (Source: Gartner AI Risk Report 2024).
* **Why It Matters:** Organizations in highly regulated industries face significant compliance risks and financial exposure, making them prime adopters of AI compliance testing solutions.

### Secondary (Future) Market Segments

**AI/LLM Developers** (AI-native firms, startups, and research institutions)

* **Estimated Market Size:** ~10,000 AI development and research institutions.
* **Why It Matters:** These firms are responsible for ensuring model compliance before deployment, making AI governance solutions a critical investment.

**Regulatory Institutions** (U.S. and international policymakers, agencies)

* **Estimated Market Size:** ~1,000 agencies and institutions.
* **Why It Matters:** Government entities can endorse, adopt, or mandate compliance testing frameworks, positioning EVAL-AI as a trusted regulatory partner.

### Competitive Landscape

EVAL-AI differentiates itself from existing solutions by offering a flexible , regulation-first compliance framework for GenAI models. 

The AI governance space requires developers and industries to be complying with required regulations. In the fast paced development associated with AI, Eval-AI provides a framework for organizations to keep up with evolving regulations. The flexibility makes Eval-AI a truly global service that can be adopted in any regulatory and even custom framework.

As the competition analysis below shows, Eval-AI’s comprehensive suite of services make it unique and extremely relevant in its offering.

![image](https://github.com/user-attachments/assets/ea4f4142-aa58-4528-9682-c0c9335d8f8e)

![image](https://github.com/user-attachments/assets/121441c3-9493-4327-a2a0-e4c842b1113c)


### User Benchmarking, Early Adoption, and Scaling Strategy

Eval-AI is positioned as the go-to compliance solution for regulated enterprises facing high AI-related risks. Our market strategy follows three key phases:

**Phase 1: Benchmarking Compliance Readiness (Phase complete)**
We assess regulatory risk, current compliance methods, and key decision-makers to refine our approach and target enterprises with the highest AI governance needs. 

**Phase 2: Targeting Early Adopters (Current phase)**
We focus on finance, healthcare, and legal industries—sectors where AI compliance is urgent. Early adoption efforts include pilot programs, regulatory partnerships, and case studies to validate Eval-AI’s impact and build credibility. 

**Phase 3: Scaling Adoption**
With proven success, we expand through enterprise partnerships, regulatory endorsements, and API integrations, while adapting Eval-AI for global compliance standards.

## Funding Plan

**Initial Funding:** $10,000 → McGinnis Social Enterprise Competition

We are adopting a **staged funding approach**, beginning with grants and social impact investments to build credibility and develop our MVP, then transitioning to a self-sustaining revenue model through enterprise adoption and regulatory partnerships.

We will **prioritize mission-aligned capital**, ensuring that compliance solutions remain accessible, independent, and impact-driven.

### Funding Model

![image](https://github.com/user-attachments/assets/c45a0b78-af86-4467-b2b4-3586c70222da)

### Revenue Model & Sustainability

Our hybrid revenue model ensures that AI compliance remains a public good, while also creating a self-sustaining social enterprise.

* AI Compliance Certification – $10K–$50K per company/year
** Mission-aligned with SOC 2 & B-Corp certification, ensuring transparency in AI governance.
* Regulatory Compliance SaaS – Sliding scale from $99/month (small firms) – $25K+/year (large enterprises)
** Equitable pricing model to ensure smaller developers and nonprofits can access compliance tools affordably.
* Public Sector & Nonprofit Partnerships – $100K–$500K per contract
** Working with governments, AI ethics organizations, and civil society groups to support AI governance at scale.
* API Licensing for AI Firms – $10K–$250K annually
** Revenue from private sector AI companies helps subsidize open-access compliance initiatives for the public.

### Social Enterprise Commitment:

Eval-AI is committed to making AI compliance accessible, equitable, and impactful. A portion of our revenue will be reinvested into AI ethics education, free compliance tools for nonprofits, and policy research on AI safety—ensuring that responsible AI is not just a corporate priority, but a societal standard. Through a freemium model, we provide free access to compliance tools for independent AI developers, journalists, and academic researchers working on Responsible AI, empowering those who hold AI systems accountable. By bridging the gap between AI innovation and public interest, Eval-AI ensures that fairness, transparency, and compliance are not just regulatory checkboxes but fundamental building blocks of ethical AI adoption worldwide.


| [Home Page](https://chrisbori.github.io/My-Online-Portfolio/) | [Eval-AI Model Check](https://chrisbori.github.io/My-Online-Portfolio/eval-ai) | [Puerto Rico's Act 22](https://chrisbori.github.io/My-Online-Portfolio/final-project-part-three) | [Visualizing Debt](https://chrisbori.github.io/My-Online-Portfolio/visualizing-government-debt) | [Everybody Poops](https://chrisbori.github.io/My-Online-Portfolio/critique-by-design) |
